// index.js
const unzipper = require('unzipper');
const { S3Client, GetObjectCommand, CreateMultipartUploadCommand, UploadPartCommand, CompleteMultipartUploadCommand, AbortMultipartUploadCommand } = require('@aws-sdk/client-s3');

const s3 = new S3Client();
const TARGET_BUCKET = process.env.TARGET_BUCKET;
const CONCURRENCY_LIMIT = 3;

exports.handler = async (event) => {
  const record = event.Records[0];
  const srcBucket = record.s3.bucket.name;
  const zipKey = decodeURIComponent(record.s3.object.key.replace(/\+/g, ' '));

  const zipStream = await getS3ObjectStream(srcBucket, zipKey);
  const directory = zipStream.pipe(unzipper.Parse());

  const fileUploadPromises = [];

  for await (const entry of directory) {
    if (entry.type === 'File') {
      fileUploadPromises.push(
        throttle(() => streamToS3(entry, entry.path), CONCURRENCY_LIMIT)
      );
    } else {
      entry.autodrain(); // skip folders
    }
  }

  // Wait for all uploads to finish
  await Promise.all(fileUploadPromises);
  console.log('All files uploaded.');
};

// Get input stream from S3
async function getS3ObjectStream(bucket, key) {
  const command = new GetObjectCommand({ Bucket: bucket, Key: key });
  const response = await s3.send(command);
  return response.Body;
}

// Upload using multipart streaming
async function streamToS3(entryStream, targetKey) {
  const upload = await s3.send(new CreateMultipartUploadCommand({
    Bucket: TARGET_BUCKET,
    Key: targetKey
  }));

  const partSize = 8 * 1024 * 1024; // 8 MB
  const parts = [];
  let partNumber = 1;
  let buffer = Buffer.alloc(0);

  return new Promise((resolve, reject) => {
    entryStream.on('data', async (chunk) => {
      buffer = Buffer.concat([buffer, chunk]);

      while (buffer.length >= partSize) {
        const partBuffer = buffer.slice(0, partSize);
        buffer = buffer.slice(partSize);

        const uploadPart = await s3.send(new UploadPartCommand({
          Bucket: TARGET_BUCKET,
          Key: targetKey,
          UploadId: upload.UploadId,
          PartNumber: partNumber,
          Body: partBuffer,
        }));

        parts.push({
          ETag: uploadPart.ETag,
          PartNumber: partNumber++
        });
      }
    });

    entryStream.on('end', async () => {
      if (buffer.length > 0) {
        const uploadPart = await s3.send(new UploadPartCommand({
          Bucket: TARGET_BUCKET,
          Key: targetKey,
          UploadId: upload.UploadId,
          PartNumber: partNumber,
          Body: buffer,
        }));
        parts.push({
          ETag: uploadPart.ETag,
          PartNumber: partNumber
        });
      }

      await s3.send(new CompleteMultipartUploadCommand({
        Bucket: TARGET_BUCKET,
        Key: targetKey,
        UploadId: upload.UploadId,
        MultipartUpload: { Parts: parts }
      }));
      console.log(`Uploaded: ${targetKey}`);
      resolve();
    });

    entryStream.on('error', async (err) => {
      await s3.send(new AbortMultipartUploadCommand({
        Bucket: TARGET_BUCKET,
        Key: targetKey,
        UploadId: upload.UploadId
      }));
      reject(err);
    });
  });
}

// Throttle concurrency
const inFlight = [];
async function throttle(fn, limit) {
  while (inFlight.length >= limit) {
    await Promise.race(inFlight);
  }
  const p = fn().finally(() => {
    inFlight.splice(inFlight.indexOf(p), 1);
  });
  inFlight.push(p);
  return p;
}
